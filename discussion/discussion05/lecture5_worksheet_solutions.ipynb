{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5 worksheet solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 (True/False)\n",
    "\n",
    "Indicate whether each of the following statements is True or False.\n",
    "\n",
    "1. An unbiased estimator is always better than a biased one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: FALSE**. if we evaluate estimators based on risk, there might be a biased estimator with lower risk than any unbiased one. This happens for instance even with Gaussian mean estimation (*see the [James-Stein paradox](https://en.wikipedia.org/wiki/Stein%27s_example)*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The Bayes risk of an estimator is a function of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: FALSE**. The *Bayesian posterior risk* is a function of the data, but the Bayes risk is just a number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. False discovery rate control is a frequentist notion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: TRUE**. In the FDR framework we introduced in this class, we do not assume any prior on either the null or non-null hypotheses. There are indeed Bayesian perspectives on this topic, but they are beyond the scope of this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 (Computing the Bayesian posterior risk)\n",
    "\n",
    "Given a loss function $l(\\theta,\\delta(X))$, the Bayesian posterior risk of an estimator $\\delta(X)$ for a parameter $\\theta$ is defined as\n",
    "\n",
    "$$\\rho(X) := \\mathbb{E}\\{l(\\theta,\\delta(X) | X\\}.$$\n",
    "\n",
    "In this question, we will compute the estimator that minimizes the posterior risk for two different loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (squared loss) Let $l(\\theta,\\delta(X)) = (\\theta-\\delta(X))^2$. What is the posterior risk minimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Let $\\delta(X)$ be the posterior risk minimizer. Then\n",
    "$$\n",
    "\\delta(X) = \\arg\\min_a \\mathbb{E}\\{(\\theta-a)^2|X\\}\n",
    "$$\n",
    "Differentiating, we get\n",
    "$$\n",
    "\\frac{d}{da}\\mathbb{E}\\{(\\theta-a)^2|X\\} = 2\\mathbb{E}\\{\\theta-a|X\\},\n",
    "$$\n",
    "and setting the derivative to be 0 gives us $a = \\mathbb{E}\\{\\theta|X\\}$, i.e. the **posterior mean**.\n",
    "\n",
    "*Remark: If working with the conditional expectation is confusing, it might be clearer to convert the expectation to an integral and do the same calculations.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (zero-one loss) Let $l(\\theta,\\delta(X)) = 1(\\theta \\neq \\delta(X))$. Assume in this case that $\\theta$ takes on a finite set of values. What is the posterior risk minimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** We have\n",
    "$$\n",
    "\\mathbb{E}\\{l(\\theta,\\delta(X) | X\\} = \\mathbb{P}\\{\\theta \\neq \\delta(X) | X\\} = 1 - \\mathbb{P}\\{\\theta = \\delta(X) | X\\}.\n",
    "$$\n",
    "\n",
    "Now note that $\\mathbb{P}\\{\\theta = \\delta(X) | X\\}$ is maximized by setting $\\delta(X)$ to be the **posterior mode**. This decision rule is also called the *Bayes classifier*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Is a minimizer of the Bayesian posterior risk also a minimizer of the Bayes risk? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Yes, a Bayesian posterior risk minimizer also minimizers the Bayes risk. This is because the Bayes risk of a procedure $\\delta(X)$ is the expectation of the Bayesian posterior risk with respect to the marginal distribution of $X$. If our procedure minimizes the posterior risk for each $X$, it therefore minimizes the Bayes risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 (Bayesian vs frequentist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a setting in which frequentist inference might be more appropriate? What is a setting in which Bayesian inference might be more appropriate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Frequentist inference is often more important in situations where safety is crucial. Randomized experiments to test the efficacy and/or safety of drugs are usually analyzed under a frequentist framework. Bayesian inference is more appropriate in settings where you want to be able to make use of expert opinion, or when you want to pull information from heterogeneous data sources. We will examine the latter setting in the upcoming lectures on Bayesian inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
